# Notes from Week of February 6, 2022 to Feburary 12, 2022

## Summary
- *2/7/2022*: Today (and yesterday) was primarily focused around learning about ramfs/rootfs/initramfs.
- *2/8/2022*: Today was pretty productive! I learned about (and refreshed my knowledge on) some of the topics that are important for making Firecracker work, namely TAP devices, Ethernet vs IP, and how iptables operates. This was all inspired by looking at [this](https://github.com/firecracker-microvm/firecracker/issues/750) issue in the FC repo and deciding to work on it.

## Context/History
So, I should probably describe _why_ I'm interested in the Firecracker microVM project in the first place.

I initially encountered the Firecracker project as part of my CS senior design project in the Fall 2020 semester. Our project sponsor, an early-stage startup focused on offering secure, multi-tenant workload capabilities aimed at government clients was trying to determine if they could retrofit a proof of concept application runtime developed by the previous senior design team to use Firecracker. The original project used Docker containers, but a combination of concerns about problems like [container escape vulnerabilities](https://thenewstack.io/what-you-need-to-know-about-the-runc-container-escape-vulnerability/) and an interest in Firecracker's no compromise promise of strong isolation and performance was enough to justify our team's foray into the unknown.

As someone who doesn't have extensive depth virtualization/containerization knowledge, I found that the _very_ compressed timeline and requirements of senior design, in combination with my other courses, made it hard to really internalize anything I learned throughout the course of the project. I also never got a chance to work as closely with Firecracker itself as I'd hoped: due to time constraints, our team decided to delegate most of the Firecracker work to a much more experienced team member, while we worked on other parts of the application runtime.

Ultimately, I finished that semester feeling unsatisfied -- I had a gnawing sense that there was _so_ much more I had to explore related to virtualization technology, as I'd always been mystified and fascinated by how any of it worked. This budding interest in virtualization coincided with a budding interest in open source software, and as Firecracker was open source (and was gaining increased attention in developer communities like Hacker News), I decided that understanding how it worked might be a good foray in learning about performant virtualization, Linux, networking, and the operation of open source projects.

To me, because the temptation to simply read code and build my mental model without _producing_ anything is so strong, I set a goal for myself of learning enough about the project to make at least one contribution upstream that closes an open [issue](https://github.com/firecracker-microvm/firecracker/issues). I think that learning is most effective when it's grounded in some sort of context, and focusing my learning around solving an existing problem would be a great way to do this. Though I primarily generally view goals as just a  _means_ to an end, rather than an end in themselves, I think that the goal I've come up with is quite achievable, but far enough out of my comfort zone that it'll push me to learn something. Also, if I am able to achieve my goal and enjoy the process, I will keep trying to contribute to this and related projects to learn more. :)

That's enough background for now, time to dive into the notes!

## Picking An Issue
Given my aforementioned issue-driven development goals, I spent a little time today (and yesterday) looking at issues that might be appropriate for me to tackle. I filtered using the ["Contribute: Good First Issue"](https://github.com/firecracker-microvm/firecracker/issues?q=is%3Aopen+is%3Aissue+label%3A%22Contribute%3A+Good+First+Issue%22) label, and started looking at what seemed interesting to me.

Ultimately, I arrived at an [issue](https://github.com/firecracker-microvm/firecracker/issues/750) opened in December 2018. At a high level, the issue is focused around producing better error messages when Firecracker receives TAP devices that it doesn't have support for -- in this case, multiqueue TAP devices.

Wanting to see if the issue was still relevant/active, I scrolled through the discussion thread and saw that the last update was in November 2020, which was around the time I was finishing senior design. The last _comment_ was in September 2020, and mentioned that there were still parties interested in this, so I decided, why not check this out?

My goal for this week is to just _understand the problem_ by giving the issue thread a thorough read through, making sure I understand the technical components involved, and can still reproduce the problem.

## Dissecting the Issue
Here's the first [comment](https://github.com/firecracker-microvm/firecracker/issues/750#issue-388477777) on the issue:
> If firecracker is passed a multiqueue tap device created using
`ip tuntap add name tap0 mode tap multi_queue`
> It reports an error
`[PUT /network-interfaces/{iface_id}][400] putGuestNetworkInterfaceByIdBadRequest  &{FaultMessage:Cannot open TAP device. Invalid name/permissions. CreateTap(
Os { code: 22, kind: InvalidInput, message: Invalid argument })}`
> Does firecracker not support multi-queue tap devices?

Upon reading this, I had _multiple_ questions:
1. What is a tap device? I vaguely remember learning about them during senior design but I don't remember what they do.
2. What is a multi-queue tap device, and how does it differ from a "normal" tap device?
3. Where is the error message being thrown from in the Firecracker code?
4. How does Firecracker use tap devices?

## Tap Devices
TODO

## `iptables`
TODO

## Ethernet vs. IP
TODO

## Getting Started with Firecracker 
I followed the Firecracker [Getting Started](https://github.com/firecracker-microvm/firecracker/blob/main/docs/getting-started.md) guide and was able to get the example described in that documentation up and running without any issues. In the process, I learned about `rootfs`, `ramfs`, and `initramfs` because the docs explained that microVM requires a `rootfs`. More information about them [here](#rootfs-ramfs-and-initramfs). While writing this, though, I realized the term `rootfs` used in the docs is not _literally_ referring to the RAM-based filesystem I learned about. It is instead just an abbreviated way of saying "root filesystem", and the actual filesystem used by the microVM in this example is `ext4`. I determined this by running `mount -l -t ext4` in a running microVM, which returned `/dev/vda on / type ext4 (rw,relatime,data=ordered)`. As I understand it, this means that a device, `/dev/vda`, is mounted to the root mount point `/` and the data in its file system is formatted based on the `ext4` file format. As `/dev/vda` is a virtualization-aware disk driver (as described [here](https://unix.stackexchange.com/questions/145332/difference-between-sdx-and-vdx)), I think this device is represents the virtualized disk that backs the root filesystem used by the microVM. Thus, all files saved on disk in the microVM will be in `ext4` format. 

### rootfs, ramfs, and initramfs
From the [source](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/Documentation/filesystems/ramfs-rootfs-initramfs.rst):
> Ramfs is a very simple filesystem that exports Linux's disk caching
mechanisms (the page cache and dentry cache) as a dynamically resizable
RAM-based filesystem.
> Normally all files are cached in memory by Linux.  Pages of data read from
backing store (usually the block device the filesystem is mounted on) are kept
around in case it's needed again, but marked as clean (freeable) in case the
Virtual Memory system needs the memory for something else.  Similarly, data
written to files is marked clean as soon as it has been written to backing
store, but kept around for caching purposes until the VM reallocates the
memory.  A similar mechanism (the dentry cache) greatly speeds up access to
directories.
> With ramfs, there is no backing store.  Files written into ramfs allocate
dentries and page cache as usual, but there's nowhere to write them to.
This means the pages are never marked clean, so they can't be freed by the
VM when it's looking to recycle memory.

So as I understand it, in the normal setup, Linux caches all files in memory for quicker access (since accessing memory is almost always faster than going to disk). File data is organized into pages, which are the smallest units of memory used by virtual memory managers. When this data is cached in memory, it can be marked as freeable in case the memory manager needs to reuse it, and presumably, a future access to that data would initially go to disk, then be cached again.

What `ramfs` does, however, is use this built-in caching mechanism to act as an in-memory filesystem (which is a series of data structures an OS uses to store and access information) without anything to "back up" or persist what's in memory. This is not an issue though, as the virtual memory manager _can't_ reallocate/free memory of this sort that doesn't have a "backing store".

Now, with that out of the way, `rootfs` is an instance of `ramfs` that is always present in the kernel, and can't be unmounted. I have a high level notion of mounting/unmounting as attaching a file system to a part of the existing system so you can access it, but I probably need to get more clarity on this.

`initramfs` is a process by which when the kernel boots up, a specially formatted archive is extracted into `rootfs`. Then, the kernel checks to see if `rootfs` contains a file named "init" and runs it with PID 1 if present; if not present, it has alternative things it can do. The init process is responsible for booting the system the rest of the way up, but I'm not clear on what else this involves at the moment.
